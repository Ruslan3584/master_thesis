\section{Субградієнтний спуск}

Цільовою функцією, яку ми будемо оптимізувати, є функція $E(\varphi)$

\begin{equation*}
    \min_{\varphi \in \Phi} E(\varphi).
\end{equation*}

% \begin{equation*}
    \begin{multline*}
    E(\varphi) = \sum\limits_{tt'\in\Gamma}\max\limits_{k\in K, k'\in K}g^{\varphi}_{tt'}(k,k') + 
    \sum\limits_{t\in T}\max\limits_{k\in K}q^{\varphi}_t(k) = \\
    =\sum\limits_{tt'\in\Gamma}\max\limits_{k\in K, k'\in K}[ g_{tt'}(k,k')
    + \varphi_{tt'}(k) + \varphi_{t't}(k)]
    +\sum\limits_{t\in T}\max\limits_{k\in K}\left[ q_t(k) - \sum\limits_{t'\in N(t)} \varphi_{tt'}(k)\right].
  \end{multline*}
% \end{equation*}
Функція $E$ є випуклою по змінній $\varphi$, проте вона є кусково-лінійною,
тому її розв'язок можна шукати за допомогою субградієнтного методу.

Субградієнтні методи \cite{Shor1985, savchynskyy,lopatka_stop_cond} --- це ітеративні методи вирішення задач опуклої оптимізації. 
Методи є збіжними навіть тоді, коли коли застосовуються навіть до недиференційованої цільової функції.
Субградієнтний спуск --- це узагальнення алгоритму градієнтного спуску \cite{chong2013introduction} для задач з недиференційованою 
цільовою функцією.

Приведемо основні етапи оптимізації методом. 
Нехай $f:\mathbb{R}^{n}\rightarrow\mathbb{R}$ є опуклою (не обов'язково диференційованою)
функцією з доменом $\mathbb{R}^{n}$.
Субградієнтний метод полягає у ітеративному повторенні кроків
\begin{equation*}
    x^{(k+1)}=x^{(k)}-\alpha_k\cdot g^{(k)},
\end{equation*}
де $g^{(k)}$ є одним із субградієнтів функції $f$ в точці $x^{(k)}$, а $k$ --- номер ітерації($k\geq0$).
Відмітимо, що якщо функція $f$ є диференційованою, то субградієнт буде співпадати із градієнтом.
Також потрібно обрати крок субградієнта $\alpha$.

Існують декілька найбільш розповсюджених способів вибору кроку.
\begin{itemize}
    \item Константний крок $\alpha_k=\alpha$, найменш розповсюджений, так як 
    при наближенні до оптимуму, при невдало обраному $\alpha$ подальший спуск буде нестабільним.
    \item Кроки, які задовольняють умовам
    $\alpha_k\geq 0$, а також щоб ряд із квадратів кроків сходився, а ряд кроків розходився, тобто 
    \begin{equation*}
        \sum\limits_{k=1}^{\infty} \alpha_k^2 < \infty, \sum\limits_{k=1}^{\infty} \alpha_k = \infty.
    \end{equation*}
    \item Послідовність спадаючих кроків, які збіжні в границі, але їх сума розбіжна, тобто
    \begin{equation*}
        \lim\limits_{k\rightarrow\infty}\alpha_k=0, \sum\limits_{k=1}^{\infty} \alpha_k = \infty.
    \end{equation*}
  \end{itemize}

Найбільш розповсюджений --- останній варіант вибору кроку субградієнта, тому в подальшому
будемо використовувати саме його.
Розглянемо найбільш загальний метод субградієнтного спуску у контексті 
використання оптимізації цільової функції $(\max,+)$ задачі розмітки.

\textbf{Ініціалізація}

\begin{enumerate}
    \item Визначимо послідовність кроків субградієнтного спуску як послідовність чисел
    $\gamma_i$, такі, що
    \begin{equation*}
        \lim\limits_{k\rightarrow\infty}\gamma_k=0, \sum\limits_{k=1}^{\infty} \gamma_k = \infty.
    \end{equation*}
    \item Для кожного ребра $((t,k),(t,k'))$, $t\in T$, $t'\in N_t$, $k\in K$, $k\in K$ введемо 
    початкові значення якостей ребер $g^0_{tt'}(k,k')=g_{tt'}(k,k')$.
    \item Для кожної вершини $(t,k)$, $t\in T$, $k\in K$ введемо початкові значення
    якостей вершин $q^0_t(k)=q_t(k)$.
    \item Присвоїти $i=0$.
\end{enumerate}

Субградієнтний спуск полягає в ітеративному обчисленні градієнтів цільової 
функції та відповідному оновленні репараметризації $\varphi$.

\textbf{Ітерації}
\begin{enumerate}
    \item Починаємо із репараметризації, яка дорівнює нулю, $\varphi_{tt'}(k)=0$, $\forall t\in T$, $t'\in N_t$, $k\in K$.
    \item Для кожної пари сусідніх об'єктів $tt'\in\Gamma$ оберемо пару міток $k\in K, k'\in K$, які утворюють ребро
    із максимальною якістю, тобто виконується 
    \begin{equation*}
        g^i_{tt'}(k,k') = \max\limits_{l\in K, l'\in K}g^i_{tt'}(l,l').
    \end{equation*}
    Оновлюємо градієнт за формулами
    \begin{equation*}
        \varphi_{tt'}(k):=\varphi_{tt'}(k)-1,
    \end{equation*}
    \begin{equation*}
        \varphi_{t't}(k'):=\varphi_{t't}(k')-1.
    \end{equation*}
    \item Для кожного об'єкту $t\in T$ оберемо мітку $k\in K$ таким чином, щоб
    \begin{equation*}
        q^i_t(k) = \max_{l\in K} q^i_t(l).
    \end{equation*}
    Для всіх сусідніх об'єктів $t'\in N_t$ оновлюємо градієнт
    \begin{equation*}
        \varphi_{tt'}(k):=\varphi_{tt'}(k)+1.
    \end{equation*}
    \item  Обчислюємо нові якості ребер $((t,k),(t',k'))$, $tt'\in\Gamma$, $k\in K$, $k'\in K$
    \begin{equation*}
        g^{i+1}_{tt'}(k,k') := g^i_{tt'}(k,k') + \varphi_{tt'}(k) + \varphi_{t't}(k').
    \end{equation*}
    \item Обчислюємо нові значення якостей вершин $(t,k)$, $t\in T$, $k\in K$
    \begin{equation*}
        q^{i+1}_t(k) :=  q^{i}_t(k) - \sum\limits_{t'N_t}\varphi_{tt'}(k).
    \end{equation*}
    \item Оновлюємо номер ітерації $i:=i+1$ і переходимо до пункту №1.
\end{enumerate}

Нехай $E(\varphi^i)$ --- якість розмітки на ітерації $i$. Тоді, згідно з теорією
методу субградієнтного спуску, справедлива рівність \cite{Shor1985}
\begin{equation*}
    \lim_{i\rightarrow\infty}E(\varphi^i) = \min_{\varphi}E(\varphi).
\end{equation*}
Тому алгоритму субградієнтного спуску вирішує задачу мінімізації, і таким чином
знаходить якість оптимальної нечіткої розмітки.

Варто зауважити, що алгоритм вимагає більш точного формулювання. Перш за все, 
умова зупинки не була вказана. Також, хоча ми й знаємо якість оптимальної розмітки, 
ми нічого не можемо сказати про саму оптимальну розмітку. Субградієнтний метод
не оперує поняттями, які можна якимось чином інтерпретувати як якість нечіткої розмітки.

Пошук строгого мінімуму --- достатньо жорстка умова, часто замість цього намагаються 
розв'язати простішу задачу --- задачу пошуку $\varepsilon$-оптимальної розмітки, тобто
\begin{equation}
    \begin{cases}
    \label{eqn:labl_relax}
      q_t(k) < \max\limits_{l\in K}q_t(l)-\varepsilon , & \implies \alpha_t(k)=0,\\
      g_{tt'}(k,k')< \max\limits_{l\in K, l'\in K}g_{tt'}(k,k')-\varepsilon, & \implies \beta_{tt'}(k,k')=0.
    \end{cases}
  \end{equation}
Існування $\varepsilon$-оптимальної розмітки не гарантує глобального оптимуму, 
але показує, що поточна якість є близькою до мінімальної якості, а також, що не менш важливо,
що поточна нечітка розмітка є близькою до шуканої оптимальної розмітки.

\textbf{Теорема.\cite{lopatka_stop_cond}} Якщо якості $(q,g)$ нечіткої розмітки із якістю $G(\varphi)$, для 
яких існує нечітка розмітка $(\alpha, \beta)$, яка задовольняє умовам (\ref{eqn:labl_relax}), 
то це означає, що справедлива наступна нерівність
\begin{equation*}
    E(\varphi)\leq E(\varphi^*) + \varepsilon\cdot (|T|+|\Gamma|).
\end{equation*}

Для визначення умови зупинки алгоритму, послабимо задачу відшукання нечіткої розмітки
\begin{equation*}
    \begin{cases}
      \sum\limits_{t\in T}\sum\limits_{t'\in N_t}\sum\limits_{k\in K}\left[\alpha_t(k)-\sum\limits_{k' \in K} \beta_{tt'}(k,k')\right]^2\leq \varepsilon,\\
      \sum\limits_{k \in K} \alpha_t(k)=1, & t\in T,\\
      \alpha_t(k)\geq 0, & t\in T, k\in K,\\
      \beta_{tt'}(k,k')\geq 0, & tt'\in \Gamma, k\in K, k'\in K.
    \end{cases}
  \end{equation*}

Відмітимо, що при $\varepsilon=0$, початкова задача, і задача з послабленою умовою
є еквівалентними.
Для кожної ітерації $i$ і отриманих значень якостей $(q^i,g^i)$ визначимо 
число $S^i$, яке назвемо нев'язкою.
\begin{equation}
    \label{eqn:slack}
    \begin{cases}
      S^i = \min\sum\limits_{t\in T}\sum\limits_{t'\in N_t}\sum\limits_{k\in K}\left[\alpha_t(k)-\sum\limits_{k' \in K} \beta_{tt'}(k,k')\right]^2,\\
      \sum\limits_{k \in K} \alpha_t(k)=1, & t\in T,\\
      \alpha_t(k)\geq 0, & t\in T, k\in K,\\
      \beta_{tt'}(k,k')\geq 0, & tt'\in \Gamma, k\in K, k'\in K,\\
      q^i_t(k)<\max\limits_{l\in K}q^i_t(l)-\varepsilon, & \implies\alpha_t(k)=0,\\
      g^i_{tt'}(k,k')< \max\limits_{l\in K, l'\in K}g^i_{tt'}(k,k')-\varepsilon, & \implies \beta_{tt'}(k,k')=0.
    \end{cases}
  \end{equation}
Таким чином, умова зупинки алгоритму субградієнтного спуску може бути визначена у термінах 
оцінки нев'язки на кожній ітерації, так, щоб нев'язка була меншою за наперед задане 
значення відхилення $\varepsilon\geq0$.

Якщо для поточних значень якості вершин та ребер $(q^i, g^i)$ значення нев'язки $S^i$ є більшим
за $\varepsilon$, то потрібно продовжувати ітерації субградієнтного спуску, якщо значення $S^i$
менше за $\varepsilon$, то це значить, що задача є достатньо близькою до тривіальної, 
і будь-яка нечітка розмітка, яка задовольняє (\ref{eqn:labl_relax}) є близькою до 
оптимальної шуканої розмітки.

Знаходження точного розв’язку до задачі (\ref{eqn:slack}) обчислювально є порівнюваною із початковою
задачею. Одначе, не обов'язково знаходити точне значення $S^i$, щоб порівняти його із $\varepsilon$, буде
достатньо знайти верхню і нижню межу $S^i$. Якщо верхня межа нев'язки $S^i$ є 
меншою за $\varepsilon$, тоді зупиняємо субградієнтний спуск. Схожим чином, якщо 
нижня межа є більшою за $\varepsilon$, тоді продовжуємо ітерувати алгоритм субградієнтного спуску.
Точний вираз для верхньої та нижньої межі можна знайти в \cite{lopatka_stop_cond}.